{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analisador Sintático HAIS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJfeqyAe1SjR"
      },
      "source": [
        "instalações necessárias\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQaJy4r1Xax"
      },
      "source": [
        "import nltk \r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import floresta\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KBfgYrstvZ"
      },
      "source": [
        "definição e treinamento do tagger com o mac-morpho\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ogYuKmpufl5"
      },
      "source": [
        "\n",
        "def simplify_tag(t):\n",
        "  if \"+\" in t:\n",
        "    return t[t.index(\"+\")+1:]\n",
        "  else:\n",
        "    return t\n",
        "\n",
        "tsents = nltk.corpus.mac_morpho.tagged_sents()\n",
        "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
        "train = tsents\n",
        "taggerMM = nltk.UnigramTagger(train)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2Q6x8M2jLT"
      },
      "source": [
        "funções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUrKnjCs2kgl"
      },
      "source": [
        "def RetiraStopWords(sentence):\r\n",
        "  stopwords = ['.',',','hoje','e','muito','frequente','frequentes','esta','de', 'o']\r\n",
        "  \r\n",
        "  phrase = []\r\n",
        "  for word in sentence:\r\n",
        "    if word not in stopwords and word !='.':\r\n",
        "      phrase.append(word)\r\n",
        "  return phrase\r\n",
        " \r\n",
        "def getApresentacao(palavra):  \r\n",
        "  retorno = palavra\r\n",
        "  \r\n",
        "  retorno = retorno.replace('<com>','<possui>')\r\n",
        "  retorno = retorno.replace('<apresenta>','<possui>')\r\n",
        "  retorno = retorno.replace('<iniciou>','<realiza>')\r\n",
        "  retorno = retorno.replace('<nega>','<**negação**>')\r\n",
        "  retorno = retorno.replace('<não>','<**negação**>')\r\n",
        "  \r\n",
        "  \r\n",
        "  return retorno.strip()  \r\n",
        "\r\n",
        "def TransformacaoIntuitiva(frase):\r\n",
        "  sentenca = frase.lower()\r\n",
        " \r\n",
        "  sentenca = sentenca.replace('com diagnóstico de','diagnosticado')\r\n",
        "  sentenca = sentenca.replace('encaminhada devido a','possui')\r\n",
        "  sentenca = sentenca.replace('encaminhado devido a','possui')\r\n",
        "  sentenca = sentenca.replace('devido a','possui')\r\n",
        "  sentenca = sentenca.replace('recebeu diagnóstico de','possui')\r\n",
        "  sentenca = sentenca.replace('em uso','usando')\r\n",
        "  sentenca = sentenca.replace('refere','possui')\r\n",
        "  sentenca = sentenca.replace('foram realizados','realizou')\r\n",
        "  sentenca = sentenca.replace('foi avaliado','avaliado')\r\n",
        "  sentenca = sentenca.replace('portador de','possui')\r\n",
        "  sentenca = sentenca.replace('com quadro de','diagnosticado')\r\n",
        "  sentenca = sentenca.replace('portadora de','diagnosticado')\r\n",
        "  sentenca = sentenca.replace('vi','6')\r\n",
        "  \r\n",
        "  return sentenca\r\n",
        "\r\n",
        "def Tokenizar(frase):\r\n",
        "  return nltk.tokenize.word_tokenize((frase.lower().replace('.',' . ')+'.').replace('..','.'))\r\n",
        "\r\n",
        "mostraLOG = False\r\n",
        "def doLOG(frase):\r\n",
        "  if mostraLOG:\r\n",
        "    print(' ')\r\n",
        "    print(frase)\r\n",
        "\r\n",
        "def ExtrairSentencas(sentencas):   \r\n",
        "  jaTeveAcao = True\r\n",
        "  fraseFinal=[]\r\n",
        "  palavrasDelimitadoras = ['com','não',',','.']\r\n",
        "  sentenca=''\r\n",
        "  for token in sentencas:\r\n",
        "    if jaTeveAcao == False:\r\n",
        "      jaTeveAcao = True\r\n",
        "      continue;\r\n",
        "    \r\n",
        "    if token[0] in palavrasDelimitadoras or ('V' in str(token[1]) ):     \r\n",
        "      if (sentenca!=''):\r\n",
        "          fraseFinal.append(sentenca.strip())\r\n",
        "          fraseFinal.append(token[0].strip())\r\n",
        "          sentenca=''\r\n",
        "          continue\r\n",
        "      else:\r\n",
        "        fraseFinal.append(token[0].strip())\r\n",
        "    else:\r\n",
        "      sentenca=sentenca+' '+token[0].strip()\r\n",
        "\r\n",
        "  return RetiraStopWords(fraseFinal)\r\n",
        "\r\n",
        "def TaggearMMorpho(tokens):\r\n",
        "  lista=[]\r\n",
        "  for token in tokens:\r\n",
        "    lista.append(taggerMM.tag([token]))\r\n",
        "\r\n",
        "  tokensRetorno=[]\r\n",
        "  for token in lista:\r\n",
        "    tokensRetorno.append(lista[0])\r\n",
        "\r\n",
        "  return lista\r\n",
        "\r\n",
        "def TransformacaoLimpeza(sentencasTaggeadas):\r\n",
        "  sentencaFinal = []\r\n",
        "  for palavra in sentencasTaggeadas:       \r\n",
        "    valor = palavra[0][0]\r\n",
        "    valorOK=''\r\n",
        "    \r\n",
        "    if (' em ' in valor):\r\n",
        "      valor = valor.split(' em ')[0]\r\n",
        "    if (' no ' in valor):\r\n",
        "      valor = valor.split(' no ')[0]  \r\n",
        "    if (' por ' in valor):\r\n",
        "      valor = valor.split(' por ')[0]  \r\n",
        "    \r\n",
        "    sentencaFinal.append([valor,palavra[0][1]])\r\n",
        "  return sentencaFinal\r\n",
        "\r\n",
        "def ExtraiTriplas(sentencasLimpas):\r\n",
        "  triplas=[]\r\n",
        "  sentencaTripla=[]\r\n",
        "  for token in sentencasLimpas:\r\n",
        "    if ('ADV' in str(token[1]) and token[0] !='não'):\r\n",
        "      continue\r\n",
        "    \r\n",
        "    if (('PREP' in str(token[1]) or \r\n",
        "         'V' in str(token[1]) or\r\n",
        "         'PCP' in str(token[1]))):\r\n",
        "      if len(sentencaTripla)>0: \r\n",
        "        triplas.append(sentencaTripla)\r\n",
        "        sentencaTripla=[token]\r\n",
        "      else:\r\n",
        "        sentencaTripla=[token]\r\n",
        "    else:\r\n",
        "      sentencaTripla.append(token)\r\n",
        "    \r\n",
        "  triplas.append(sentencaTripla)\r\n",
        "  return triplas\r\n",
        "\r\n",
        "def ExtraiPredicado(triplas):\r\n",
        "  triplaFinal=[]\r\n",
        "  numLoop=0\r\n",
        "  for tripla in triplas:\r\n",
        "    numLoop=0\r\n",
        "    predicado=''\r\n",
        "\r\n",
        "    for sentenca in tripla:\r\n",
        "      if numLoop==0:\r\n",
        "        predicado=sentenca[0]\r\n",
        "      numLoop+=1\r\n",
        "      if numLoop>1:\r\n",
        "        saida='<Paciente> ' +predicado+'<'+sentenca[0]+'> '\r\n",
        "        triplaFinal.append([predicado,sentenca[0]])\r\n",
        "\r\n",
        "  return triplaFinal\r\n",
        "\r\n",
        "def LimparPredicados(predicados):\r\n",
        "  triplaCorreta=[]\r\n",
        "  for tripla in predicados:\r\n",
        "\r\n",
        "    tokensPredicado = nltk.tokenize.word_tokenize(tripla[1])\r\n",
        "    tagPredicado = taggerMM.tag(tokensPredicado)\r\n",
        "    \r\n",
        "    if ('PREP' in str(tagPredicado[0][1]) or 'PROADJ' in str(tagPredicado[0][1])):\r\n",
        "      continue\r\n",
        "\r\n",
        "    if (('era' in tripla[0]) or ('ocup'  in tripla[0])):\r\n",
        "      continue\r\n",
        "    \r\n",
        "    triplaCorreta.append(tripla)\r\n",
        "  return triplaCorreta\r\n",
        "\r\n",
        "def Concatenar(lista):\r\n",
        "  retorno=''\r\n",
        "  for item in lista:\r\n",
        "    retorno+=item+' '\r\n",
        "  return retorno\r\n",
        "\r\n",
        "def ExtrairRDF(predicado):\r\n",
        "  numLoop=0\r\n",
        "  alvo = Concatenar(RetiraStopWords(Tokenizar(predicado[1])))\r\n",
        "  saida='<Paciente> ' +getApresentacao('<'+predicado[0]+'>')+ ' <'+getApresentacao(alvo)+'> '\r\n",
        "        \r\n",
        "  return saida\r\n",
        "\r\n",
        "def TaggearTokens(tokens):\r\n",
        "  tokensPreTaggeadas=[]\r\n",
        "  for token in tokens:\r\n",
        "    tokensPreTaggeadas.append(taggerMM.tag([token]))\r\n",
        "\r\n",
        "  doLOG('TOKENS PRÉ TAGGEADAS:')\r\n",
        "  doLOG(tokensPreTaggeadas)\r\n",
        "\r\n",
        "  tokensTaggeadas = []\r\n",
        "  for token in tokensPreTaggeadas:\r\n",
        "    tokensTaggeadas.append(token[0])    \r\n",
        "  return tokensTaggeadas\r\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZECf9x5gU1u"
      },
      "source": [
        "\n",
        "def analiseTeste(label,sentenca):\n",
        "  doLOG('#######')\n",
        "  doLOG('ORIGINAL:' + sentenca)  \n",
        "\n",
        "  sentencaTransformada = TransformacaoIntuitiva(sentenca)\n",
        "\n",
        "  tokens = Tokenizar(sentencaTransformada)\n",
        "  \n",
        "  tokensTaggeadas = TaggearTokens(tokens)  \n",
        "  doLOG('TOKENS TAGGEADAS:')\n",
        "  doLOG(tokensTaggeadas)\n",
        "\n",
        "  sentencas = ExtrairSentencas(tokensTaggeadas)\n",
        "  doLOG('SENTENCAS:')\n",
        "  doLOG(sentencas)\n",
        "    \n",
        "  sentencasTaggeadas = TaggearMMorpho(sentencas)\n",
        "  doLOG('SENTENCAS TAGGEADAS:')\n",
        "  doLOG(sentencasTaggeadas)\n",
        "  \n",
        "  sentencasLimpas = TransformacaoLimpeza(sentencasTaggeadas)\n",
        "  doLOG('SENTENCAS LIMPAS:')\n",
        "  doLOG(sentencasLimpas)\n",
        "  \n",
        "  triplas = ExtraiTriplas(sentencasLimpas)\n",
        "  doLOG('TRIPLAS:')\n",
        "  doLOG(triplas)  \n",
        "  \n",
        "  triplasPredicado = ExtraiPredicado(triplas)\n",
        "  doLOG('PREDICADOS:')\n",
        "  doLOG(triplasPredicado)  \n",
        "\n",
        "  predicadosLimpos = LimparPredicados(triplasPredicado)\n",
        "  doLOG('PREDICADOS LIMPOS:')\n",
        "  doLOG(predicadosLimpos)  \n",
        "  \n",
        "  print('')\n",
        "  print('TRIPLAS historia #'+str(label))  \n",
        "  for predicado in predicadosLimpos:\n",
        "    fraseRDF = ExtrairRDF(predicado)    \n",
        "    print(fraseRDF)\n",
        "   \n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-4b3rbuZEM",
        "outputId": "0a8c41f7-efc8-4178-b941-c0666465b096"
      },
      "source": [
        "analiseTeste(1,'Paciente com HEMOFILIA A GRAVE em PROFILAXIA SECUNDÁRIA. Vem hoje com queixa de dor \\\n",
        "                em dorso do pé, nega traumas locais')\n",
        "analiseTeste(2,'Paciente com Hemofilia A grave')\n",
        "analiseTeste(3,'Paciente com Mucopolissacaridose tipo VI, Síndrome de Marateaux Lami iniciou terapia de \\\n",
        "                repozição enzimática em 30/04/2013')\n",
        "analiseTeste(4,'Paciente com hemangioma em região toracica á direita, desde o nascimento. Essa lesão era \\\n",
        "                bem extensa ocupando 2/3 do hemitorax D , vem regredindo gradativamente')\n",
        "analiseTeste(5,'Pciente portador de ANEMIA FALCIFORME,acompanha neste seviço desde o nascimento.Apresenta \\\n",
        "                crises dolorosas muito frequentes,internaçoes múltiplas,necessidade transfusional frequente. \\\n",
        "                Retorna para controle.Sofreu queimadura no ombro direito por bolsa de agua quente devido á dor,\\\n",
        "                com bolhas e sem infecção local.Esta com dor moderada no corpo todo')\n",
        "analiseTeste(6,' Paciente com diagnóstico de meduloblastoma por anatomopatologico de cirurgia com ressecção \\\n",
        "                macroscópica incompleta em 08/08/12.      Não foi avaliado coluna e LCR negativo.')\n",
        "analiseTeste(7,'É encaminhada devido a leucopenia (exame de origem leu=2.900). Queixa-se de dores em ambos os\\\n",
        "                 joelhos, sobretudo ao deambular. Sintomas agravaram-se após ganho de peso de quase 15Kg. Passou\\\n",
        "                 ontem por nutricionista e recebeu as orientações dietéticas cabíveis.Realizou acompanhamento na\\\n",
        "                  Hematologia até jan/2012, durante o qual recebeu diagnóstico de trombofilia.Hb=11,8  Ht=36,2%  \\\n",
        "                  Leu=3.040  Plaqu=182.000')\n",
        "analiseTeste(8,'Vem encaminhada devido a plaquetopenia. Fev=100mil  Maio=60mil. Tem relato de equimoses espontâneas.')\n",
        "analiseTeste(9,'Refere manchas no corpo todo, há 1 ano. Refere surgimento de feridas pruriginosas no corpo, \\\n",
        "                inicialmente em face, mãos e pernas, que progrediram para todo o corpo. Alega que fez tratamento \\\n",
        "                com ebastel e pomada de corticoide que nao recorda. Foram realizados exames para investigação de \\\n",
        "                alergias alimentares, sem alterações importantes, sugestivas. ')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRIPLAS historia #1\n",
            "<Paciente> <possui> <hemofilia a grave> \n",
            "<Paciente> <possui> <queixa dor> \n",
            "<Paciente> <**negação**> <traumas locais> \n",
            "\n",
            "TRIPLAS historia #2\n",
            "<Paciente> <possui> <hemofilia a grave> \n",
            "\n",
            "TRIPLAS historia #3\n",
            "<Paciente> <possui> <mucopolissacaridose tipo 6> \n",
            "<Paciente> <possui> <síndrome marateaux lami> \n",
            "<Paciente> <realiza> <terapia repozição enzimática> \n",
            "\n",
            "TRIPLAS historia #4\n",
            "<Paciente> <possui> <hemangioma> \n",
            "\n",
            "TRIPLAS historia #5\n",
            "<Paciente> <possui> <anemia falciforme> \n",
            "<Paciente> <acompanha> <neste se6ço desde nascimento> \n",
            "<Paciente> <possui> <crises dolorosas> \n",
            "<Paciente> <possui> <internaçoes múltiplas> \n",
            "<Paciente> <possui> <necessidade transfusional> \n",
            "<Paciente> <sofreu> <queimadura> \n",
            "<Paciente> <possui> <bolhas sem infecção local> \n",
            "<Paciente> <possui> <dor moderada> \n",
            "\n",
            "TRIPLAS historia #6\n",
            "<Paciente> <possui> <ressecção macroscópica incompleta> \n",
            "<Paciente> <**negação**> <avaliado coluna lcr negativo> \n",
            "\n",
            "TRIPLAS historia #7\n",
            "<Paciente> <possui> <leucopenia ( exame origem leu=2> \n",
            "<Paciente> <possui> <900 )> \n",
            "<Paciente> <possui> <queixa-se dores> \n",
            "<Paciente> <possui> <sintomas agravaram-se após ganho peso> \n",
            "<Paciente> <possui> <15kg> \n",
            "<Paciente> <passou> <por nutricionista> \n",
            "<Paciente> <recebeu> <as orientações dietéticas cabíveis> \n",
            "<Paciente> <realizou> <acompanhamento na hematologia até jan/2012> \n",
            "<Paciente> <possui> <trombofilia> \n",
            "<Paciente> <possui> <hb=11,8 ht=36,2 % leu=3> \n",
            "<Paciente> <possui> <040 plaqu=182> \n",
            "<Paciente> <possui> <000> \n",
            "\n",
            "TRIPLAS historia #8\n",
            "<Paciente> <possui> <plaquetopenia> \n",
            "<Paciente> <possui> <fev=100mil maio=60mil> \n",
            "<Paciente> <tem> <relato equimoses espontâneas> \n",
            "\n",
            "TRIPLAS historia #9\n",
            "<Paciente> <possui> <manchas> \n",
            "<Paciente> <há> <1 ano> \n",
            "<Paciente> <possui> <surgimento feridas pruriginosas> \n",
            "<Paciente> <possui> <em face> \n",
            "<Paciente> <possui> <mãos pernas> \n",
            "<Paciente> <possui> <que progrediram para todo corpo> \n",
            "<Paciente> <alega> <que> \n",
            "<Paciente> <fez> <tratamento> \n",
            "<Paciente> <possui> <ebastel pomada corticoide que> \n",
            "<Paciente> <realizou> <exames para investigação alergias alimentares> \n",
            "<Paciente> <realizou> <sugestivas> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22R9A5sWPNOy"
      },
      "source": [
        "import functools"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g_WKHzeRvDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e458b2-cc29-4006-9829-e0457b0f5046"
      },
      "source": [
        "  compose = lambda *F: functools.reduce(lambda f, g: lambda x: f(g(x)), F)\r\n",
        "\r\n",
        "  g = compose(TaggearTokens,\r\n",
        "              Tokenizar,\r\n",
        "              TransformacaoIntuitiva)\r\n",
        "  print(g('Paciente possui hemofilia'))\r\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('paciente', 'N'), ('possui', 'V'), ('hemofilia', None), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPItczmuu9WU"
      },
      "source": [
        "def Stemar(frase):\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  tokens = nltk.tokenize.word_tokenize((frase.lower()))\n",
        "  frasesStemadas = []\n",
        "  palavraStemada=''\n",
        "  palavraNormal=''\n",
        "  for token in tokens:\n",
        "    frasesStemadas.append((stemmer.stem(token),token))\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "    palavraNormal += token+ ' '\n",
        "  frasesStemadas.append((palavraStemada,palavraNormal))\n",
        "\n",
        "  palavraStemada=''\n",
        "  palavraNormal=''\n",
        "  cont =1 \n",
        "  for token in tokens:\n",
        "    if cont<2:\n",
        "      cont+=1\n",
        "      continue;\n",
        "      \n",
        "    frasesStemadas.append((stemmer.stem(token),token))\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "    palavraNormal += token+ ' '\n",
        "    frasesStemadas.append((palavraStemada,palavraNormal))\n",
        "\n",
        "  palavraStemada=''\n",
        "  palavraNormal=''\n",
        "  cont =1 \n",
        "  for token in tokens:\n",
        "    if cont<3:\n",
        "      cont+=1\n",
        "      continue;\n",
        "      \n",
        "    frasesStemadas.append((stemmer.stem(token),token))\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "    palavraNormal += token+ ' '\n",
        "    frasesStemadas.append((palavraStemada,palavraNormal))\n",
        "\n",
        "  palavraStemada=''\n",
        "  palavraNormal=''\n",
        "  cont =1 \n",
        "  for token in tokens:\n",
        "    if cont<4:\n",
        "      cont+=1\n",
        "      continue;\n",
        "      \n",
        "    frasesStemadas.append((stemmer.stem(token),token))\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "    palavraNormal += token+ ' '\n",
        "    frasesStemadas.append((palavraStemada,palavraNormal))\n",
        "\n",
        "  palavraStemada=''\n",
        "  palavraNormal=''\n",
        "  cont =1 \n",
        "  for token in tokens[:-1]:\n",
        "    if cont<2:\n",
        "      cont+=1\n",
        "      continue;\n",
        "      \n",
        "    frasesStemadas.append((stemmer.stem(token),token))\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "    palavraNormal += token+ ' '\n",
        "    frasesStemadas.append((palavraStemada,palavraNormal))\n",
        "\n",
        "  palavraStemada=''\n",
        "  palavraNormal=''\n",
        "  cont =1 \n",
        "  for token in tokens[:-1]:\n",
        "    if cont<1:\n",
        "      cont+=1\n",
        "      continue;\n",
        "      \n",
        "    frasesStemadas.append((stemmer.stem(token),token))\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "    palavraNormal += token+ ' '\n",
        "    frasesStemadas.append((palavraStemada,palavraNormal))\n",
        "\n",
        "  return frasesStemadas\n",
        "\n",
        "def StemarUnitario(frase):\n",
        "  stemmer = nltk.stem.RSLPStemmer()\n",
        "  tokens = nltk.tokenize.word_tokenize((frase.lower()))\n",
        "  frasesStemadas = []\n",
        "  palavraStemada=''\n",
        "  for token in tokens:\n",
        "    palavraStemada += stemmer.stem(token)\n",
        "\n",
        "\n",
        "  return palavraStemada"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F7ZSoVC2Hht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a7006a-0e6c-453a-a40a-57d55e6c042b"
      },
      "source": [
        "\r\n",
        "def simplify_tag(t):\r\n",
        "  if \"+\" in t:\r\n",
        "    return t[t.index(\"+\")+1:]\r\n",
        "  else:\r\n",
        "    return t\r\n",
        "\r\n",
        "tsents = nltk.corpus.mac_morpho.tagged_sents()\r\n",
        "\r\n",
        "stemmer = nltk.stem.RSLPStemmer()\r\n",
        "\r\n",
        "dataset = [[(StemarUnitario('dor de cabeça'),'sintoma'),\r\n",
        "            (StemarUnitario('trombofilia'),'sintoma'),\r\n",
        "            (StemarUnitario('necessidade transfusão'),'procedimento')\r\n",
        "            ]]\r\n",
        "\r\n",
        "print(dataset)\r\n",
        "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in dataset if sent]\r\n",
        "train = tsents\r\n",
        "taggerHC = nltk.UnigramTagger(train)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('dordecabeç', 'sintoma'), ('trombofil', 'sintoma'), ('necesstransfus', 'procedimento')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3mxfzsIrj6I"
      },
      "source": [
        "def ScanPredicado(frase):\n",
        "  frasesStemadas = Stemar(frase.replace('<Paciente>','').replace('<','').replace('>',''))\n",
        " \n",
        "  for frase in frasesStemadas:\n",
        "    tokens = taggerHC.tag([frase[0]])\n",
        "    #print(tokens)\n",
        "    for token in tokens:\n",
        "      #print(token[1])\n",
        "      if (token[1] is not None):\n",
        "        #print(token)\n",
        "        return (token,frase[1].strip())\n",
        "  #print('fin')\n",
        "  return None"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1pRuXZxlzLP",
        "outputId": "e3520e17-9a40-4a1b-f412-bb39843cc2dd"
      },
      "source": [
        "frases = []\r\n",
        "frases.append('<paciente> <possui> <dor de cabeça>')\r\n",
        "frases.append('<paciente> <tem> <dor de cabeça aguda>')\r\n",
        "frases.append('<paciente> <relatou> <muitas dores de cabeça>')\r\n",
        "frases.append('<paciente> <informou> <dores de cabeça matinais>')\r\n",
        "frases.append('<paciente> <nega> <dores de cabeça>')\r\n",
        "\r\n",
        "for frase in frases:\r\n",
        "  print(ScanPredicado(frase))\r\n",
        "\r\n",
        "#tokens = Tokenizar(frase)   \r\n",
        "#tokens = RetiraStopWords(tokens)\r\n",
        "#tokensRadical=[]\r\n",
        "#for token in tokens:\r\n",
        "#  tokensRadical.append(stemmer.stem(token))\r\n",
        "#print(tokensRadical)\r\n",
        "\r\n"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('dordecabeç', 'sintoma'), 'dor de cabeça')\n",
            "(('dordecabeç', 'sintoma'), 'dor de cabeça')\n",
            "(('dordecabeç', 'sintoma'), 'dores de cabeça')\n",
            "(('dordecabeç', 'sintoma'), 'dores de cabeça')\n",
            "(('dordecabeç', 'sintoma'), 'dores de cabeça')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUVqWuVumDnd"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgqMUaxYGPGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ea8e3902-9cc3-48d7-c12e-de60194b5022"
      },
      "source": [
        "stemmer.stem(\"copiar\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'copi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZBvhB0g3XW0"
      },
      "source": [
        "array = ['banana', 'maça', 'uva','pera']"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Flds6ZxE3bgE",
        "outputId": "ad8c8e89-b741-42da-ca97-e513204cefb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "array[:-1]"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['banana', 'maça', 'uva']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGv7K52b3dSB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}