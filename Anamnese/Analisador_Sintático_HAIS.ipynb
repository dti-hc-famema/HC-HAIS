{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Analisador Sintático HAIS.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJfeqyAe1SjR"
      },
      "source": [
        "instalações necessárias\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQaJy4r1Xax"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('all')\r\n",
        "from nltk.corpus import floresta\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0KBfgYrstvZ"
      },
      "source": [
        "definição e treinamento do tagger com o mac-morpho\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ogYuKmpufl5"
      },
      "source": [
        "\n",
        "def simplify_tag(t):\n",
        "  if \"+\" in t:\n",
        "    return t[t.index(\"+\")+1:]\n",
        "  else:\n",
        "    return t\n",
        "\n",
        "tsents = nltk.corpus.mac_morpho.tagged_sents()\n",
        "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in tsents if sent]\n",
        "train = tsents\n",
        "taggerMM = nltk.UnigramTagger(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cq2Q6x8M2jLT"
      },
      "source": [
        "funções"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUrKnjCs2kgl"
      },
      "source": [
        "def RetiraStopWords(sentence):\r\n",
        "  stopwords = ['.',',','hoje','e','muito','frequente','frequentes','esta','de', 'o']\r\n",
        "  \r\n",
        "  phrase = []\r\n",
        "  for word in sentence:\r\n",
        "    if word not in stopwords and word !='.':\r\n",
        "      phrase.append(word)\r\n",
        "  return phrase\r\n",
        " \r\n",
        "def getApresentacao(palavra):  \r\n",
        "  retorno = palavra\r\n",
        "  \r\n",
        "  retorno = retorno.replace('<com>','<possui>')\r\n",
        "  retorno = retorno.replace('<apresenta>','<possui>')\r\n",
        "  retorno = retorno.replace('<iniciou>','<realiza>')\r\n",
        "  retorno = retorno.replace('<nega>','<**negação**>')\r\n",
        "  retorno = retorno.replace('<não>','<**negação**>')\r\n",
        "  \r\n",
        "  \r\n",
        "  return retorno.strip()  \r\n",
        "\r\n",
        "def TransformacaoIntuitiva(frase):\r\n",
        "  sentenca = frase.lower()\r\n",
        "  \r\n",
        "  sentenca = sentenca.replace('com diagnóstico de','possui')\r\n",
        "  sentenca = sentenca.replace('encaminhada devido a','possui')\r\n",
        "  sentenca = sentenca.replace('encaminhado devido a','possui')\r\n",
        "  sentenca = sentenca.replace('devido a','possui')\r\n",
        "  sentenca = sentenca.replace('recebeu diagnóstico de','possui')\r\n",
        "  sentenca = sentenca.replace('em uso','usando')\r\n",
        "  sentenca = sentenca.replace('refere','possui')\r\n",
        "  sentenca = sentenca.replace('foram realizados','realizou')\r\n",
        "  sentenca = sentenca.replace('foi avaliado','avaliado')\r\n",
        "  sentenca = sentenca.replace('portador de','possui')\r\n",
        "  sentenca = sentenca.replace('com quadro de','diagnosticado')\r\n",
        "  sentenca = sentenca.replace('portadora de','diagnosticado')\r\n",
        "  sentenca = sentenca.replace('vi','6')\r\n",
        "\r\n",
        "  return sentenca\r\n",
        "\r\n",
        "def Tokenizar(frase):\r\n",
        "  return nltk.tokenize.word_tokenize((frase.lower().replace('.',' . ')+'.').replace('..','.'))\r\n",
        "\r\n",
        "mostraLOG = False\r\n",
        "def doLOG(frase):\r\n",
        "  if mostraLOG:\r\n",
        "    print(' ')\r\n",
        "    print(frase)\r\n",
        "\r\n",
        "def ExtrairSentencas(sentencas):   \r\n",
        "  jaTeveAcao = True\r\n",
        "  fraseFinal=[]\r\n",
        "  palavrasDelimitadoras = ['com','não',',','.']\r\n",
        "  sentenca=''\r\n",
        "  for token in sentencas:\r\n",
        "    if jaTeveAcao == False:\r\n",
        "      jaTeveAcao = True\r\n",
        "      continue;\r\n",
        "    \r\n",
        "    if token[0] in palavrasDelimitadoras or ('V' in str(token[1]) ):     \r\n",
        "      if (sentenca!=''):\r\n",
        "          fraseFinal.append(sentenca.strip())\r\n",
        "          fraseFinal.append(token[0].strip())\r\n",
        "          sentenca=''\r\n",
        "          continue\r\n",
        "      else:\r\n",
        "        fraseFinal.append(token[0].strip())\r\n",
        "    else:\r\n",
        "      sentenca=sentenca+' '+token[0].strip()\r\n",
        "\r\n",
        "  return RetiraStopWords(fraseFinal)\r\n",
        "\r\n",
        "def TaggearMMorpho(tokens):\r\n",
        "  lista=[]\r\n",
        "  for token in tokens:\r\n",
        "    lista.append(taggerMM.tag([token]))\r\n",
        "\r\n",
        "  tokensRetorno=[]\r\n",
        "  for token in lista:\r\n",
        "    tokensRetorno.append(lista[0])\r\n",
        "\r\n",
        "  return lista\r\n",
        "\r\n",
        "def TransformacaoLimpeza(sentencasTaggeadas):\r\n",
        "  sentencaFinal = []\r\n",
        "  for palavra in sentencasTaggeadas:       \r\n",
        "    valor = palavra[0][0]\r\n",
        "    valorOK=''\r\n",
        "    \r\n",
        "    if (' em ' in valor):\r\n",
        "      valor = valor.split(' em ')[0]\r\n",
        "    if (' no ' in valor):\r\n",
        "      valor = valor.split(' no ')[0]  \r\n",
        "    if (' por ' in valor):\r\n",
        "      valor = valor.split(' por ')[0]  \r\n",
        "    \r\n",
        "    sentencaFinal.append([valor,palavra[0][1]])\r\n",
        "  return sentencaFinal\r\n",
        "\r\n",
        "def ExtraiTriplas(sentencasLimpas):\r\n",
        "  triplas=[]\r\n",
        "  sentencaTripla=[]\r\n",
        "  for token in sentencasLimpas:\r\n",
        "    if ('ADV' in str(token[1]) and token[0] !='não'):\r\n",
        "      continue\r\n",
        "    \r\n",
        "    if (('PREP' in str(token[1]) or \r\n",
        "         'V' in str(token[1]) or\r\n",
        "         'PCP' in str(token[1]))):\r\n",
        "      if len(sentencaTripla)>0: \r\n",
        "        triplas.append(sentencaTripla)\r\n",
        "        sentencaTripla=[token]\r\n",
        "      else:\r\n",
        "        sentencaTripla=[token]\r\n",
        "    else:\r\n",
        "      sentencaTripla.append(token)\r\n",
        "    \r\n",
        "  triplas.append(sentencaTripla)\r\n",
        "  return triplas\r\n",
        "\r\n",
        "def ExtraiPredicado(triplas):\r\n",
        "  triplaFinal=[]\r\n",
        "  numLoop=0\r\n",
        "  for tripla in triplas:\r\n",
        "    numLoop=0\r\n",
        "    predicado=''\r\n",
        "\r\n",
        "    for sentenca in tripla:\r\n",
        "      if numLoop==0:\r\n",
        "        predicado=sentenca[0]\r\n",
        "      numLoop+=1\r\n",
        "      if numLoop>1:\r\n",
        "        saida='<Paciente> ' +predicado+'<'+sentenca[0]+'> '\r\n",
        "        triplaFinal.append([predicado,sentenca[0]])\r\n",
        "\r\n",
        "  return triplaFinal\r\n",
        "\r\n",
        "def LimparPredicados(predicados):\r\n",
        "  triplaCorreta=[]\r\n",
        "  for tripla in predicados:\r\n",
        "\r\n",
        "    tokensPredicado = nltk.tokenize.word_tokenize(tripla[1])\r\n",
        "    tagPredicado = taggerMM.tag(tokensPredicado)\r\n",
        "    \r\n",
        "    if ('PREP' in str(tagPredicado[0][1]) or 'PROADJ' in str(tagPredicado[0][1])):\r\n",
        "      continue\r\n",
        "\r\n",
        "    if (('era' in tripla[0]) or ('ocup'  in tripla[0])):\r\n",
        "      continue\r\n",
        "    \r\n",
        "    triplaCorreta.append(tripla)\r\n",
        "  return triplaCorreta\r\n",
        "\r\n",
        "def Concatenar(lista):\r\n",
        "  retorno=''\r\n",
        "  for item in lista:\r\n",
        "    retorno+=item+' '\r\n",
        "  return retorno\r\n",
        "\r\n",
        "def ExtrairRDF(predicado):\r\n",
        "  numLoop=0\r\n",
        "  alvo = Concatenar(RetiraStopWords(Tokenizar(predicado[1])))\r\n",
        "  saida='<Paciente> ' +getApresentacao('<'+predicado[0]+'>')+ ' <'+getApresentacao(alvo)+'> '\r\n",
        "        \r\n",
        "  return saida\r\n",
        "\r\n",
        "def TaggearTokens(tokens):\r\n",
        "  tokensPreTaggeadas=[]\r\n",
        "  for token in tokens:\r\n",
        "    tokensPreTaggeadas.append(taggerMM.tag([token]))\r\n",
        "\r\n",
        "  doLOG('TOKENS PRÉ TAGGEADAS:')\r\n",
        "  doLOG(tokensPreTaggeadas)\r\n",
        "\r\n",
        "  tokensTaggeadas = []\r\n",
        "  for token in tokensPreTaggeadas:\r\n",
        "    tokensTaggeadas.append(token[0])    \r\n",
        "  return tokensTaggeadas\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZECf9x5gU1u"
      },
      "source": [
        "\n",
        "def analiseTeste(label,sentenca):\n",
        "  doLOG('#######')\n",
        "  doLOG('ORIGINAL:' + sentenca)  \n",
        "\n",
        "  sentencaTransformada = TransformacaoIntuitiva(sentenca)\n",
        "\n",
        "  tokens = Tokenizar(sentencaTransformada)\n",
        "  \n",
        "  tokensTaggeadas = TaggearTokens(tokens)  \n",
        "  doLOG('TOKENS TAGGEADAS:')\n",
        "  doLOG(tokensTaggeadas)\n",
        "\n",
        "  sentencas = ExtrairSentencas(tokensTaggeadas)\n",
        "  doLOG('SENTENCAS:')\n",
        "  doLOG(sentencas)\n",
        "    \n",
        "  sentencasTaggeadas = TaggearMMorpho(sentencas)\n",
        "  doLOG('SENTENCAS TAGGEADAS:')\n",
        "  doLOG(sentencasTaggeadas)\n",
        "  \n",
        "  sentencasLimpas = TransformacaoLimpeza(sentencasTaggeadas)\n",
        "  doLOG('SENTENCAS LIMPAS:')\n",
        "  doLOG(sentencasLimpas)\n",
        "  \n",
        "  triplas = ExtraiTriplas(sentencasLimpas)\n",
        "  doLOG('TRIPLAS:')\n",
        "  doLOG(triplas)  \n",
        "  \n",
        "  triplasPredicado = ExtraiPredicado(triplas)\n",
        "  doLOG('PREDICADOS:')\n",
        "  doLOG(triplasPredicado)  \n",
        "\n",
        "  predicadosLimpos = LimparPredicados(triplasPredicado)\n",
        "  doLOG('PREDICADOS LIMPOS:')\n",
        "  doLOG(predicadosLimpos)  \n",
        "  \n",
        "  print('')\n",
        "  print('TRIPLAS historia #'+str(label))  \n",
        "  for predicado in predicadosLimpos:\n",
        "    fraseRDF = ExtrairRDF(predicado)    \n",
        "    print(fraseRDF)\n",
        "   \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hL-4b3rbuZEM",
        "outputId": "2d3f433a-ff2f-4c12-8c92-c92c4f51c271"
      },
      "source": [
        "analiseTeste(1,'Paciente com HEMOFILIA A GRAVE em PROFILAXIA SECUNDÁRIA. Vem hoje com queixa de dor \\\n",
        "                em dorso do pé, nega traumas locais')\n",
        "analiseTeste(2,'Paciente com Hemofilia A grave')\n",
        "analiseTeste(3,'Paciente com Mucopolissacaridose tipo VI, Síndrome de Marateaux Lami iniciou terapia de \\\n",
        "                repozição enzimática em 30/04/2013')\n",
        "analiseTeste(4,'Paciente com hemangioma em região toracica á direita, desde o nascimento. Essa lesão era \\\n",
        "                bem extensa ocupando 2/3 do hemitorax D , vem regredindo gradativamente')\n",
        "analiseTeste(5,'Pciente portador de ANEMIA FALCIFORME,acompanha neste seviço desde o nascimento.Apresenta \\\n",
        "                crises dolorosas muito frequentes,internaçoes múltiplas,necessidade transfusional frequente. \\\n",
        "                Retorna para controle.Sofreu queimadura no ombro direito por bolsa de agua quente devido á dor,\\\n",
        "                com bolhas e sem infecção local.Esta com dor moderada no corpo todo')\n",
        "analiseTeste(6,' Paciente com diagnóstico de meduloblastoma por anatomopatologico de cirurgia com ressecção \\\n",
        "                macroscópica incompleta em 08/08/12.      Não foi avaliado coluna e LCR negativo.')\n",
        "analiseTeste(7,'É encaminhada devido a leucopenia (exame de origem leu=2.900). Queixa-se de dores em ambos os\\\n",
        "                 joelhos, sobretudo ao deambular. Sintomas agravaram-se após ganho de peso de quase 15Kg. Passou\\\n",
        "                 ontem por nutricionista e recebeu as orientações dietéticas cabíveis.Realizou acompanhamento na\\\n",
        "                  Hematologia até jan/2012, durante o qual recebeu diagnóstico de trombofilia.Hb=11,8  Ht=36,2%  \\\n",
        "                  Leu=3.040  Plaqu=182.000')\n",
        "analiseTeste(8,'Vem encaminhada devido a plaquetopenia. Fev=100mil  Maio=60mil. Tem relato de equimoses espontâneas.')\n",
        "analiseTeste(9,'Refere manchas no corpo todo, há 1 ano. Refere surgimento de feridas pruriginosas no corpo, \\\n",
        "                inicialmente em face, mãos e pernas, que progrediram para todo o corpo. Alega que fez tratamento \\\n",
        "                com ebastel e pomada de corticoide que nao recorda. Foram realizados exames para investigação de \\\n",
        "                alergias alimentares, sem alterações importantes, sugestivas. ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "TRIPLAS historia #1\n",
            "<Paciente> <possui> <hemofilia a grave> \n",
            "<Paciente> <possui> <queixa dor> \n",
            "<Paciente> <**negação**> <traumas locais> \n",
            "\n",
            "TRIPLAS historia #2\n",
            "<Paciente> <possui> <hemofilia a grave> \n",
            "\n",
            "TRIPLAS historia #3\n",
            "<Paciente> <possui> <mucopolissacaridose tipo 6> \n",
            "<Paciente> <possui> <síndrome marateaux lami> \n",
            "<Paciente> <realiza> <terapia repozição enzimática> \n",
            "\n",
            "TRIPLAS historia #4\n",
            "<Paciente> <possui> <hemangioma> \n",
            "\n",
            "TRIPLAS historia #5\n",
            "<Paciente> <possui> <anemia falciforme> \n",
            "<Paciente> <acompanha> <neste se6ço desde nascimento> \n",
            "<Paciente> <possui> <crises dolorosas> \n",
            "<Paciente> <possui> <internaçoes múltiplas> \n",
            "<Paciente> <possui> <necessidade transfusional> \n",
            "<Paciente> <sofreu> <queimadura> \n",
            "<Paciente> <possui> <bolhas sem infecção local> \n",
            "<Paciente> <possui> <dor moderada> \n",
            "\n",
            "TRIPLAS historia #6\n",
            "<Paciente> <possui> <meduloblastoma> \n",
            "<Paciente> <possui> <ressecção macroscópica incompleta> \n",
            "<Paciente> <**negação**> <avaliado coluna lcr negativo> \n",
            "\n",
            "TRIPLAS historia #7\n",
            "<Paciente> <possui> <leucopenia ( exame origem leu=2> \n",
            "<Paciente> <possui> <900 )> \n",
            "<Paciente> <possui> <queixa-se dores> \n",
            "<Paciente> <possui> <sintomas agravaram-se após ganho peso> \n",
            "<Paciente> <possui> <15kg> \n",
            "<Paciente> <passou> <por nutricionista> \n",
            "<Paciente> <recebeu> <as orientações dietéticas cabíveis> \n",
            "<Paciente> <realizou> <acompanhamento na hematologia até jan/2012> \n",
            "<Paciente> <possui> <trombofilia> \n",
            "<Paciente> <possui> <hb=11,8 ht=36,2 % leu=3> \n",
            "<Paciente> <possui> <040 plaqu=182> \n",
            "<Paciente> <possui> <000> \n",
            "\n",
            "TRIPLAS historia #8\n",
            "<Paciente> <possui> <plaquetopenia> \n",
            "<Paciente> <possui> <fev=100mil maio=60mil> \n",
            "<Paciente> <tem> <relato equimoses espontâneas> \n",
            "\n",
            "TRIPLAS historia #9\n",
            "<Paciente> <possui> <manchas> \n",
            "<Paciente> <há> <1 ano> \n",
            "<Paciente> <possui> <surgimento feridas pruriginosas> \n",
            "<Paciente> <possui> <em face> \n",
            "<Paciente> <possui> <mãos pernas> \n",
            "<Paciente> <possui> <que progrediram para todo corpo> \n",
            "<Paciente> <alega> <que> \n",
            "<Paciente> <fez> <tratamento> \n",
            "<Paciente> <possui> <ebastel pomada corticoide que> \n",
            "<Paciente> <realizou> <exames para investigação alergias alimentares> \n",
            "<Paciente> <realizou> <sugestivas> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22R9A5sWPNOy"
      },
      "source": [
        "import functools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g_WKHzeRvDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8744efaf-c2ce-40d4-9948-75ac9788ee98"
      },
      "source": [
        "  compose = lambda *F: functools.reduce(lambda f, g: lambda x: f(g(x)), F)\r\n",
        "\r\n",
        "  g = compose(TaggearTokens,\r\n",
        "              Tokenizar,\r\n",
        "              TransformacaoIntuitiva)\r\n",
        "  print(g('Paciente possui hemofilia'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('paciente', 'N'), ('possui', 'V'), ('hemofilia', None), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F7ZSoVC2Hht",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "272f7003-f3c9-4174-b6d7-65c95b353385"
      },
      "source": [
        "\r\n",
        "def simplify_tag(t):\r\n",
        "  if \"+\" in t:\r\n",
        "    return t[t.index(\"+\")+1:]\r\n",
        "  else:\r\n",
        "    return t\r\n",
        "\r\n",
        "tsents = nltk.corpus.mac_morpho.tagged_sents()\r\n",
        "\r\n",
        "stemmer = nltk.stem.RSLPStemmer()\r\n",
        "\r\n",
        "dataset = [[(stemmer.stem('dor'),'sintoma'),\r\n",
        "            (stemmer.stem('cirurgia'),'procedimento'),\r\n",
        "            (stemmer.stem('allan'),'craque'),\r\n",
        "            (stemmer.stem('regis'),'grosso'),\r\n",
        "            (stemmer.stem('nelson'),'chefe'),\r\n",
        "            (stemmer.stem('gabriel'),'grosso'),\r\n",
        "            (stemmer.stem('renam'),'grosso'),\r\n",
        "            (stemmer.stem('hc'),'unidade'),\r\n",
        "            (stemmer.stem('hemocentro'),'unidade'),\r\n",
        "            (stemmer.stem('dor de cabeça'),'unidade'),\r\n",
        "            ]]\r\n",
        "\r\n",
        "print(dataset)\r\n",
        "tsents = [[(w.lower(),simplify_tag(t)) for (w,t) in sent] for sent in dataset if sent]\r\n",
        "train = tsents\r\n",
        "taggerHC = nltk.UnigramTagger(train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[('dor', 'sintoma'), ('cirurg', 'procedimento'), ('allan', 'craque'), ('regil', 'grosso'), ('nelson', 'chefe'), ('gabriel', 'grosso'), ('ren', 'grosso'), ('hc', 'unidade'), ('hemocentr', 'unidade'), ('dor de cabeç', 'unidade')]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1pRuXZxlzLP",
        "outputId": "99799956-95e2-4261-a877-b17ac2ae03ca"
      },
      "source": [
        "frase = 'allan, regis, gabriel e rena trabalham no hc, sob supervisão do nelson. \\\r\n",
        "         eles não trabalham no hemocentro e nem no lucy. os pacientes do hc, \\\r\n",
        "         apresentam algum tipo de dor.alguns fazem cirurgi'\r\n",
        "\r\n",
        "tokens = Tokenizar(frase)   \r\n",
        "tokens = RetiraStopWords(tokens)\r\n",
        "tokensRadical=[]\r\n",
        "for token in tokens:\r\n",
        "  tokensRadical.append(stemmer.stem(token))\r\n",
        "\r\n",
        "tokens = taggerHC.tag(tokensRadical)\r\n",
        "for token in tokens:\r\n",
        "  if (token[1] is not None):\r\n",
        "    print(token)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('allan', 'craque')\n",
            "('regil', 'grosso')\n",
            "('gabriel', 'grosso')\n",
            "('ren', 'grosso')\n",
            "('hc', 'unidade')\n",
            "('nelson', 'chefe')\n",
            "('hemocentr', 'unidade')\n",
            "('hc', 'unidade')\n",
            "('dor', 'sintoma')\n",
            "('cirurg', 'procedimento')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUVqWuVumDnd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgqMUaxYGPGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "915dd19c-d89a-4f22-bf34-20925287487f"
      },
      "source": [
        "stemmer.stem(\"copiar\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'copi'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}